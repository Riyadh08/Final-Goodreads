{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1751ade-d59a-4566-ae57-7d92f717b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Read the `temp` list from the `books_with_ids.csv` file\n",
    "books_with_ids = pd.read_csv('final_book_url.csv')\n",
    "temp = books_with_ids['Book_URL'].tolist()  # Assuming the file has a column named 'urls'\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Configure retries\n",
    "retries = Retry(total=5,  # Retry up to 5 times\n",
    "                backoff_factor=1,  # Wait 1 second between retries\n",
    "                status_forcelist=[500, 502, 503, 504],  # Retry on specific status codes\n",
    "                raise_on_status=False)\n",
    "\n",
    "# Mount the retry adapter to the session\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "m_urls = []  # List to store all URLs\n",
    "visited = set()  # Set to track visited URLs\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "for x in temp:\n",
    "    try:\n",
    "        # Skip if the URL was already visited\n",
    "        \n",
    "\n",
    "        # Use the session to make requests with retry logic\n",
    "        response = session.get(x, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "        \n",
    "        # Loop through each div and extract the href from the <a> tag\n",
    "        for div in tt:\n",
    "            a_tag = div.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href not in visited:  # Check if the href is already visited\n",
    "                    m_urls.append(href)\n",
    "                    visited.add(href)  # Mark this href as visited\n",
    "\n",
    "       # print(f\"Processed {x}: Extracted {len(m_urls)} total URLs so far.\")\n",
    "\n",
    "        # Mark the current URL as visited\n",
    "        #visited.add(x)\n",
    "\n",
    "        # Introduce a delay of 1 second between requests to avoid server overloading\n",
    "        time.sleep(1)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for: {x}, error: {e}\")\n",
    "\n",
    "# Save the extracted URLs to a new CSV file called 'bengali_users.csv'\n",
    "df = pd.DataFrame(m_urls, columns=['URLs'])\n",
    "df.to_csv('bengali_users.csv', index=False)\n",
    "\n",
    "print(f\"Total URLs extracted: {len(m_urls)}\")\n",
    "print(\"Scraping completed. Data saved to 'bengali_users.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae533c63-491a-44b2-bc25-da33a2d7e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for: https://www.goodreads.com/book/show/75613852-anapus, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/18490745, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/58089567, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/205818432---bangabandhu-muktijuddho, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/170004951, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/53857617, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/168364474-smritikatha-o-anyanya, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/200924413, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/16269542-vol-12, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/170244563-golpogulu-sob-valo, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/53262873, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Request failed for: https://www.goodreads.com/book/show/219969553-3-stories---set-in-india, error: HTTPSConnectionPool(host='www.goodreads.com', port=443): Read timed out.\n",
      "Total URLs extracted: 9383\n",
      "Scraping completed. Data saved to 'bengali_users.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Read the `temp` list from the `books_with_ids.csv` file\n",
    "books_with_ids = pd.read_csv('final_book_url.csv')\n",
    "temp = books_with_ids['Book_URL'].tolist()  # Assuming the file has a column named 'urls'\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Configure retries\n",
    "retries = Retry(total=5,  # Retry up to 5 times\n",
    "                backoff_factor=1,  # Wait 1 second between retries\n",
    "                status_forcelist=[500, 502, 503, 504],  # Retry on specific status codes\n",
    "                raise_on_status=False)\n",
    "\n",
    "# Mount the retry adapter to the session\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "m_urls = []  # List to store all URLs\n",
    "visited = set()  # Set to track visited URLs\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def scrape_url(x):\n",
    "    \"\"\"Scrape a single URL and extract relevant data.\"\"\"\n",
    "    local_m_urls = []\n",
    "    try:\n",
    "        # Use the session to make requests with retry logic\n",
    "        response = session.get(x, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tt = soup.find_all('div', class_='ReviewerProfile__name')\n",
    "\n",
    "        # Loop through each div and extract the href from the <a> tag\n",
    "        for div in tt:\n",
    "            a_tag = div.find('a')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                href = a_tag['href']\n",
    "                if href not in visited:  # Check if the href is already visited\n",
    "                    local_m_urls.append(href)\n",
    "                    visited.add(href)  # Mark this href as visited\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for: {x}, error: {e}\")\n",
    "    return local_m_urls\n",
    "\n",
    "# Use ThreadPoolExecutor for multithreading\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust max_workers as needed\n",
    "    futures = {executor.submit(scrape_url, url): url for url in temp}\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            m_urls.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL: {futures[future]}, error: {e}\")\n",
    "\n",
    "# Save the extracted URLs to a new CSV file called 'bengali_users.csv'\n",
    "df = pd.DataFrame(m_urls, columns=['URLs'])\n",
    "df.to_csv('bengali_users.csv', index=False)\n",
    "\n",
    "print(f\"Total URLs extracted: {len(m_urls)}\")\n",
    "print(\"Scraping completed. Data saved to 'bengali_users.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cfc0a-1668-49c8-aed1-0bc4bfdab39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b8210-718d-449b-877a-a42886a332d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
